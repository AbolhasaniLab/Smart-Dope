{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b71d7e",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal, HeUniform, HeNormal, Zeros, Ones\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import linregress\n",
    "from joblib import Parallel, delayed\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a09c1a",
   "metadata": {},
   "source": [
    "### Real-Time Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d09c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set of functions used to read and process the raw spectra\n",
    "def read_files(loc):\n",
    "    files_all = sorted(os.listdir(loc), key = len)\n",
    "    ABS_names = []\n",
    "    PL_names = []\n",
    "    ABS_all = []\n",
    "    PL_all = []\n",
    "    for file in files_all:\n",
    "        if (file.startswith(\"Abs\")):\n",
    "            ABS = pd.read_csv(loc + \"//\" + file , header = None).to_numpy()\n",
    "            ABS_all.append(ABS)\n",
    "            ABS_names.append(file.split(\".\")[0])\n",
    "        elif (file.startswith(\"PL\")):\n",
    "            PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "            PL_all.append(PL)\n",
    "            PL_names.append(file.split(\".\")[0])\n",
    "        elif(file.startswith(\"Wavelength_Abs\")):\n",
    "            WL_ABS = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"Wavelength_PL\")):\n",
    "            WL_PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"DR_Abs\")):\n",
    "            DR_ABS = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"DR_PL\")):\n",
    "            DR_PL = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"LR\")):\n",
    "            LR = pd.read_csv(loc + \"//\" + file, header = None).to_numpy()\n",
    "        elif(file.startswith(\"FR\")):\n",
    "            FR = pd.read_csv(loc + \"//\" + file, header = None).to_numpy() # FR is the file including experimental conditions \n",
    "    return WL_ABS, WL_PL, DR_ABS, DR_PL, LR, ABS_all, PL_all, ABS_names, PL_names, FR\n",
    "def idx_min(y, x):\n",
    "    diff = np.abs(y[:, 0] - x)\n",
    "    idx = diff.argmin()\n",
    "    return idx\n",
    "# extract reactive phase\n",
    "def extract(x, idx1, idx2, numFirstElements):\n",
    "    x_mean = x[idx1:idx2 + 1, :].mean(axis = 0)\n",
    "    x_sort = np.sort(x_mean)\n",
    "    idx_phase = np.nonzero(np.in1d(x_mean, x_sort[:numFirstElements]))[0]\n",
    "    x_phase = x[:, idx_phase]\n",
    "    return x_phase\n",
    "def spectra_avg(x):\n",
    "    x_avg = x.mean(axis = 1) # avg over extracted reactive phase spectra for each WL; will result in a row matrix\n",
    "    x_avg = x_avg[:, np.newaxis]\n",
    "    return x_avg\n",
    "def baseline_zero(x, idx_low, idx_high):\n",
    "    x_baseline_zero = x - x[idx_low:idx_high + 1, :].mean() # make baseline zero; subtracting mean of spectra at LL - HL nm\n",
    "    return x_baseline_zero\n",
    "def linear_int_x(y, x, i, y_btw):\n",
    "    x_btw = x[i, 0] - ((x[i + 1, 0] - x[i, 0]) / (y[i + 1, 0] - y[i, 0])) * (y[i, 0] - y_btw)\n",
    "    return x_btw\n",
    "def linear_int_y(y, x, i, x_btw):\n",
    "    y_btw = y[i, 0] - ((y[i + 1, 0] - y[i, 0]) / (x[i + 1, 0] - x[i, 0])) * (x[i, 0] - x_btw)\n",
    "    return y_btw\n",
    "def peak_info(y, x):\n",
    "    area_peak = np.trapz(y.T, x = x.T)[0] # emission peak area\n",
    "    return area_peak\n",
    "# process ABS\n",
    "def spectra_extract_ABS(WL, DR, LR, ABS_all, ABS_names):\n",
    "    # baseline WL limits\n",
    "    baseline_LL = 550\n",
    "    baseline_HL = 600\n",
    "    idx_WL_LL = idx_min(WL, baseline_LL)\n",
    "    idx_WL_HL = idx_min(WL, baseline_HL)\n",
    "    # to extract reactive phase\n",
    "    lastPeak_LL = 310\n",
    "    lastPeak_HL = 340 \n",
    "    idx_lastPeak_LL = idx_min(WL, lastPeak_LL)\n",
    "    idx_lastPeak_HL = idx_min(WL, lastPeak_HL)\n",
    "    # excitation WL info\n",
    "    excitation_WL = 365\n",
    "    idx_excitationWL = idx_min(WL, excitation_WL)\n",
    "    # initiate files to be exported\n",
    "    WLABS_processed = WL\n",
    "    ABS_excitationWL_list = []\n",
    "    # ABS_excitationWL_list = np.array([[\"Absorbance at excitation WL\"]])\n",
    "    for (item, name) in zip(ABS_all, ABS_names):\n",
    "        ABS_only = item[1:, :]\n",
    "        ## extract reactive phase and remove carrier phase (PFO)\n",
    "        param = 100 # number to average\n",
    "        ABS_phase = extract(ABS_only, idx_lastPeak_LL, idx_lastPeak_HL, param)\n",
    "        ABS_phase_avg = spectra_avg(ABS_phase) \n",
    "        arg_log = (ABS_phase_avg - DR) / (LR - DR) # Beer-Lambert law\n",
    "        ABS_processed = - np.log10(arg_log) # Beer-Lambert law\n",
    "        ABS_processed_baseline = baseline_zero(ABS_processed, idx_WL_LL, idx_WL_HL) \n",
    "        WLABS_processed = np.concatenate( [WLABS_processed, ABS_processed_baseline] , axis = 1) # attach WL and processed ABS\n",
    "        # WLABS_processed_filtered = WLABS_processed[np.isnan(WLABS_processed[:, 1]) == False] # remove NaN ABS\n",
    "        ABS_excitationWL = linear_int_y(ABS_processed_baseline, WL, idx_excitationWL, excitation_WL)\n",
    "        ABS_excitationWL_list.append(ABS_excitationWL)\n",
    "        # ABS_excitationWL_list = np.concatenate( [ABS_excitationWL_list, np.array([ABS_excitationWL])[:, np.newaxis]] , axis = 1)\n",
    "    ABS_excitationWL_list = np.array(ABS_excitationWL_list)[:, np.newaxis]\n",
    "    return ABS_excitationWL_list\n",
    "# process PL\n",
    "def spectra_extract_PL(WL, DR, PL_all, PL_names):\n",
    "    ## baseline WL limits\n",
    "    baseline_LL = 1050\n",
    "    baseline_HL = 1100\n",
    "    idx_WL_LL = idx_min(WL, baseline_LL)\n",
    "    idx_WL_HL = idx_min(WL, baseline_HL)\n",
    "    # to extract reactive phase\n",
    "    excWL_LL = 315\n",
    "    excWL_HL = 415\n",
    "    idx_excWL_LL = idx_min(WL, excWL_LL)\n",
    "    idx_excWL_HL = idx_min(WL, excWL_HL)\n",
    "    # emission peak 1 WL limits\n",
    "    emPeak1_LL = 395 \n",
    "    emPeak1_HL = 450\n",
    "    idx_emPeak1_LL = idx_min(WL, emPeak1_LL)\n",
    "    idx_emPeak1_HL = idx_min(WL, emPeak1_HL)\n",
    "    WL_aroundPeak1 = WL[idx_emPeak1_LL: idx_emPeak1_HL + 1, :]\n",
    "    # emission peak 2 WL limits\n",
    "    emPeak2_LL = 550\n",
    "    emPeak2_HL = 800\n",
    "    idx_emPeak2_LL = idx_min(WL, emPeak2_LL)\n",
    "    idx_emPeak2_HL = idx_min(WL, emPeak2_HL)\n",
    "    WL_aroundPeak2 = WL[idx_emPeak2_LL: idx_emPeak2_HL + 1, :]\n",
    "    # emission peak 3 WL limits\n",
    "    emPeak3_LL = 900 \n",
    "    emPeak3_HL = 1100\n",
    "    idx_emPeak3_LL = idx_min(WL, emPeak3_LL)\n",
    "    idx_emPeak3_HL = idx_min(WL, emPeak3_HL)\n",
    "    WL_aroundPeak3 = WL[idx_emPeak3_LL: idx_emPeak3_HL + 1, :]\n",
    "    ## initiate files to be exported\n",
    "    WLPL_processed = WL\n",
    "    emission_areas = []\n",
    "    # emission_details = np.array([[\"Emission Peak WL\"], [\"Emission Peak Intensity\"], [\"FWHM\"], [\"Emission Peak Area\"]])\n",
    "    for (item, name) in zip(PL_all, PL_names):\n",
    "        PL_only = item[1:, :]\n",
    "        ## extract reactive phase and remove carrier phase (PFO)\n",
    "        param = 50\n",
    "        PL_phase = extract(PL_only, idx_excWL_LL, idx_excWL_HL, param) \n",
    "        PL_phase_avg = spectra_avg(PL_phase)\n",
    "        PL_processed = PL_phase_avg - DR\n",
    "        PL_processed_baseline = baseline_zero(PL_processed, idx_WL_LL, idx_WL_HL) \n",
    "        WLPL_processed = np.concatenate( [WLPL_processed, PL_processed_baseline] , axis = 1) # attach WL and processed PL\n",
    "        ## emission peak 1 area\n",
    "        PL_aroundPeak1 = PL_processed_baseline[idx_emPeak1_LL: idx_emPeak1_HL + 1, :]\n",
    "        area_peak1 = peak_info(PL_aroundPeak1, WL_aroundPeak1)\n",
    "        ## emission peak 2 area\n",
    "        PL_aroundPeak2 = PL_processed_baseline[idx_emPeak2_LL: idx_emPeak2_HL + 1, :]\n",
    "        area_peak2 = peak_info(PL_aroundPeak2, WL_aroundPeak2)\n",
    "        ## emission peak 3 area\n",
    "        PL_aroundPeak3 = PL_processed_baseline[idx_emPeak3_LL: idx_emPeak3_HL + 1, :]\n",
    "        area_peak3 = peak_info(PL_aroundPeak3, WL_aroundPeak3)\n",
    "        areas_arr = np.array([[area_peak1], [area_peak2], [area_peak3]])\n",
    "        emission_areas.append(areas_arr)\n",
    "    emission_areas = np.concatenate(emission_areas, axis = 1).T\n",
    "    return emission_areas\n",
    "## calculate relative PLQY\n",
    "def rel_PLQY(ABS_sample, areas_sample):\n",
    "    # data related to the dye and solvent\n",
    "    PLQY_dye = 0.675\n",
    "    ABS_dye = 0.15781\n",
    "#     area1_dye = 1447050\n",
    "    area2_dye = 205117.25 * 3\n",
    "    area3_dye = 16094.91 * 2.2\n",
    "    etha_dye = 1.337\n",
    "    etha_ODE = 1.444\n",
    "    # calculate relative_PLQY\n",
    "#     PLQY1 = PLQY_dye * (areas_sample[:, [0]] * ABS_dye * etha_ODE) / (area1_dye * ABS_sample * etha_dye)\n",
    "    PLQY2 = PLQY_dye * (areas_sample[:, [0]] * ABS_dye * etha_ODE) / (area2_dye * ABS_sample * etha_dye)\n",
    "    PLQY3 = PLQY_dye * (areas_sample[:, [1]] * ABS_dye * etha_ODE) / (area3_dye * ABS_sample * etha_dye)\n",
    "    PLQY_2_3 = PLQY2 + PLQY3\n",
    "#     relative_PLQY = PLQY1 + PLQY2 + PLQY3\n",
    "    # return relative PLQY\n",
    "    return PLQY_2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca01c736",
   "metadata": {},
   "source": [
    "### ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale output\n",
    "def non_dim_y(y_initial):\n",
    "    y_scaled = np.zeros((y_initial.shape[0], y_initial.shape[1]))\n",
    "    y_scaled[:, 0] = (y_initial[:, 0]) / (2 * 0.25)\n",
    "    y_scaled[:, 1] = (y_initial[:, 1]) / (2 * 360476)\n",
    "    y_scaled[:, 2] = (y_initial[:, 2]) / (2 * 71768)\n",
    "    return y_scaled\n",
    "## scale output back\n",
    "def dim_y(y_initial):\n",
    "    y_scaled = np.zeros((y_initial.shape[0], y_initial.shape[1]))\n",
    "    y_scaled[:, 0] = (y_initial[:, 0]) * (2 * 0.25)\n",
    "    y_scaled[:, 1] = (y_initial[:, 1]) * (2 * 360476)\n",
    "    y_scaled[:, 2] = (y_initial[:, 2]) * (2 * 71768)\n",
    "    return y_scaled\n",
    "## nondimensionalize inputs\n",
    "def non_dim_x(x_initial):\n",
    "    x_normalized = np.zeros((x_initial.shape[0], x_initial.shape[1] - 3))\n",
    "    x_normalized[:, 0] = (x_initial[:, 0] - 160) / (255 - 160) # x1 = temperature\n",
    "    x_normalized[:, 1] = (x_initial[:, 1] - 45) / (50 - 45) # x2 = ODE\n",
    "    x_normalized[:, 2] = (x_initial[:, 2] - 10) / (30 - 10) # x3 = OA\n",
    "    x_normalized[:, 3] = (x_initial[:, 3] - 10) / (30 - 10) # x4 = OAm\n",
    "    x_normalized[:, 4] = (x_initial[:, 4] - 20) / (70 - 20) # x5 = Cl\n",
    "    x_normalized[:, 5] = (x_initial[:, 5] - 25) / (80 - 25) # x6 = Yb\n",
    "    x_normalized[:, 6] = (x_initial[:, 6] - 10) / (40 - 10) # x7 = Mn\n",
    "    x_normalized[:, 7] = (x_initial[:, 7] - 15) / (50 - 15) # x8 = Pb\n",
    "    x_normalized[:, 8] = (x_initial[:, 8] - 15) / (50 - 15) # x8 = Cs\n",
    "    return x_normalized\n",
    "## split dataset to training and validation\n",
    "def split_set(x, y, train_ratio = 0.8, seed_num = 100):\n",
    "    xy_all = np.concatenate((x, y), axis = 1)\n",
    "    np.random.seed(seed_num)  \n",
    "    np.random.shuffle(xy_all)\n",
    "    # training set\n",
    "    xy_train = xy_all[:int(train_ratio * xy_all.shape[0]), :]\n",
    "    x_train = xy_train[:, :-y.shape[1]]\n",
    "    y_train = xy_train[:, -y.shape[1]:]\n",
    "    # validation set\n",
    "    xy_test = xy_all[int(train_ratio * xy_all.shape[0]):, :]\n",
    "    x_test = xy_test[:, :-y.shape[1]]\n",
    "    y_test = xy_test[:, -y.shape[1]:]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "## ENN structure\n",
    "def structure_info(num_models, layer_min = 8, layer_max = 10, node_min = 15, node_max = 30, seed_num = 100):\n",
    "    random.seed(seed_num)\n",
    "    detail_all = []\n",
    "    for i in range (num_models):\n",
    "        num_layers = random.randint(layer_min, layer_max)\n",
    "        nodes_list = []\n",
    "        for j in range(num_layers):\n",
    "            num_nodes = random.randint(node_min, node_max)\n",
    "            nodes_list.append(num_nodes)\n",
    "        detail = [num_layers, nodes_list]\n",
    "        detail_all.append(detail)\n",
    "    return detail_all\n",
    "## build and train an ensemble of cascade NNs\n",
    "def parallel_train_ML(train_x, train_y, val_x, val_y, structure):\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    ## cascade NN\n",
    "    input = Input(shape = train_x.shape[1])\n",
    "    x = input\n",
    "    for layer in range(structure[0]):\n",
    "        output = Dense(structure[1][layer], activation = 'relu', kernel_initializer = HeUniform(seed = 42), bias_initializer = Zeros())(x)\n",
    "        x = Concatenate()([x, output])\n",
    "    # dropout = Dropout(0.1)(x)\n",
    "    # output = Dense(train_y.shape[1], activation = 'linear')(dropout)\n",
    "    output = Dense(train_y.shape[1], activation = 'linear')(x)\n",
    "    model = Model(inputs = input, outputs = output)\n",
    "    ## train the model\n",
    "    # early stopping terminates training if validation model loss begins to increase due to over-fitting\n",
    "    # early_stopping = EarlyStopping(monitor = 'val_loss', patience = 50, min_delta = 1e-5, mode = 'min', restore_best_weights = True)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 50, min_delta = 1e-5, mode = 'min')\n",
    "    # model compile with loss, optimizer, and performance metric of interest\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = RMSprop(learning_rate = 1e-4), metrics = [RootMeanSquaredError()])\n",
    "    # perform model training with training and validation data with early stopping\n",
    "    history = model.fit(train_x, train_y, validation_data = (val_x, val_y), epochs = 1000, verbose = 0, callbacks = [early_stopping])\n",
    "    result = [model, history]\n",
    "    return result\n",
    "## deconvolute the model and training history\n",
    "def deconvolute_ML(ensemble_result):\n",
    "    ensemble_model = []\n",
    "    ensemble_history = []\n",
    "    for element in ensemble_result:\n",
    "        ensemble_model.append(element[0])\n",
    "        ensemble_history.append(element[1])\n",
    "    return ensemble_model, ensemble_history\n",
    "## plots related to the training process\n",
    "def learn_plot(history):\n",
    "    fig = plt.figure()\n",
    "    # loss values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], color = 'blue')\n",
    "    plt.plot(history.history['val_loss'], color = 'red')\n",
    "    plt.title('Loss (MSE) - Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    # metric values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['root_mean_squared_error'], color = 'blue')\n",
    "    plt.plot(history.history['val_root_mean_squared_error'], color = 'red')\n",
    "    plt.title('RMSE - Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "## plot related to the predicted vs. true outputs - for individual models   \n",
    "def predict_plot_separate(model, x, y_true):\n",
    "    predicted_y = model.predict(x, verbose = 0)\n",
    "    detail_1 = linregress(y_true[:, 0], predicted_y[:, 0])\n",
    "    detail_2 = linregress(y_true[:, 1], predicted_y[:, 1])\n",
    "    detail_3 = linregress(y_true[:, 2], predicted_y[:, 2])\n",
    "#     detail_4 = linregress(y_true[:, 3], predicted_y[:, 3])\n",
    "    plt.scatter(y_true[:, 0], predicted_y[:, 0], color = 'blue')\n",
    "    plt.scatter(y_true[:, 1], predicted_y[:, 1], color = 'red')\n",
    "    plt.scatter(y_true[:, 2], predicted_y[:, 2], color = 'cyan')\n",
    "#     plt.scatter(y_true[:, 3], predicted_y[:, 3], color = 'yellow')\n",
    "    plt.plot([0, 1],[0, 1], color = 'green')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    # plt.legend(['y0: $R^2 = $' + str(detail_1.rvalue ** 2), 'y1: $R^2 = $' + str(detail_2.rvalue ** 2), 'y2: $R^2 = $' + str(detail_3.rvalue ** 2), 'y3: $R^2 = $' + str(detail_4.rvalue ** 2), 'Parity'])\n",
    "    plt.legend(['y0: $R^2 = $' + str(detail_1.rvalue ** 2), 'y1: $R^2 = $' + str(detail_2.rvalue ** 2), 'y2: $R^2 = $' + str(detail_3.rvalue ** 2), 'Parity'])\n",
    "    plt.show()\n",
    "## plot related to the predicted vs. true outputs - for the entire ENN   \n",
    "def predict_plot_mean(ensemble_NN, x, y_true):\n",
    "    y_pred = []\n",
    "    for model in ensemble_NN:\n",
    "        y_pred.append(model.predict(x, verbose = 0))\n",
    "    y_pred_arr = np.concatenate(y_pred, axis = 1)\n",
    "    dim = y_true.shape[1]\n",
    "    y_pred_mean = []\n",
    "    y_pred_std = []\n",
    "    plt.figure(figsize = (3.5 * dim, 3.5), dpi = 200)\n",
    "    for i in range(dim):\n",
    "        element = y_pred_arr[:, i::dim]\n",
    "        element_mean = np.mean(element, axis = 1)\n",
    "        element_std = np.std(element, axis = 1)\n",
    "        element_detail = linregress(y_true[:, i], element_mean)\n",
    "        y_pred_mean.append(element_mean[:, np.newaxis])\n",
    "        y_pred_std.append(element_std[:, np.newaxis])\n",
    "        plt.subplot(1, dim, i + 1)\n",
    "        plt.errorbar(y_true[:, i], element_mean, yerr = element_std, fmt = 'o', color = 'blue', alpha = 0.8, markersize = 5, label = 'y' + str(i) + ': $R^2 = $' + str(round((element_detail.rvalue ** 2), 4)))\n",
    "        plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "        # plt.title('y' + str(i + 1))\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    pred_mean = np.concatenate(y_pred_mean, axis = 1)\n",
    "    pred_std = np.concatenate(y_pred_std, axis = 1)\n",
    "    return pred_mean, pred_std\n",
    "## save the ENN in the given path\n",
    "def save_ENN(ENN, path):\n",
    "    for n, model in enumerate(ENN):\n",
    "        os.mkdir(path + \"//\" + str(n))\n",
    "        model.save(path + \"//\" + str(n))\n",
    "## load the ENN stored in the given path\n",
    "def load_plot(path):\n",
    "    model_idx = sorted(os.listdir(path), key = len)\n",
    "    ensemble = []\n",
    "    for idx in model_idx:\n",
    "        ensemble.append(load_model(path + \"//\" + idx))\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c609acc",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used of expected improvement (EI)\n",
    "def best_target_data(y_norm, y_target):\n",
    "    obj_fun_data = abs(y_norm - y_target)\n",
    "    obj_fun_best = obj_fun_data.min()\n",
    "    return obj_fun_best\n",
    "## aquisition functions implementation\n",
    "def apply_policy(x_initial, ensemble, y_target, obj_fun_best, policy):\n",
    "    # p = psutil.Process()\n",
    "    # p.nice(psutil.REALTIME_PRIORITY_CLASS)    \n",
    "    x = x_initial[np.newaxis, :]\n",
    "    obj_models = []\n",
    "    for model in ensemble:\n",
    "        y_pred = model.predict(x, verbose = 0)\n",
    "        y_pred_dim = dim_y(y_pred)\n",
    "        combination_pred = rel_PLQY(y_pred_dim[:, [0]], y_pred_dim[:, 1:]) / 2\n",
    "        # our ptoblem is single objective optimization\n",
    "        obj_fun = abs(y_target - np.squeeze(combination_pred)[()])\n",
    "        obj_models.append(obj_fun)\n",
    "    ## EI\n",
    "    if (policy[0] == \"EI\"):\n",
    "        imp_list = []\n",
    "        for element in obj_models:\n",
    "            diff = obj_fun_best - element \n",
    "            if (diff >= 0):\n",
    "                imp_list.append(diff)\n",
    "            else:\n",
    "                imp_list.append(0)\n",
    "        value = - np.mean(imp_list)\n",
    "    ## UCB\n",
    "    if (policy[0] == \"UCB\"):\n",
    "        value = np.mean(obj_models) - (policy[1] * np.std(obj_models))\n",
    "    ## MV      \n",
    "    if (policy[0] == \"MV\"):\n",
    "        value = - np.std(obj_models)\n",
    "    ## EPLT\n",
    "    elif (policy[0] == \"EPLT\"):\n",
    "        value =  np.mean(obj_models)\n",
    "    return value\n",
    "## optimization\n",
    "def opt_general(f, dim):\n",
    "    # p = psutil.Process()\n",
    "    # p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    low_limit = [0] * dim\n",
    "    up_limit = [1] * dim\n",
    "    bnds = list(zip(low_limit, up_limit))\n",
    "    count_success = 0\n",
    "    while (count_success < 1):\n",
    "        x_initial = np.random.uniform(0, 1, dim)\n",
    "        # sol = minimize(f, x_initial, method = \"trust-constr\", bounds = bnds, options = {'gtol': 0.02, 'xtol': 0.02, 'verbose': 0})\n",
    "        sol = minimize(f, x_initial, method = \"SLSQP\", bounds = bnds, options = {'ftol': 0.02, 'maxiter': 1000})\n",
    "        if ((sol.success == True) and (sol.nit > 1)):\n",
    "            result = sol\n",
    "            count_success = count_success + 1 \n",
    "    return result      \n",
    "def parallel_opt(ensemble, y_target, obj_fun_best, policy, x_shape):\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    model_pol = lambda x: apply_policy(x, ensemble, y_target, obj_fun_best, policy)\n",
    "    solution = opt_general(model_pol, x_shape)\n",
    "    return solution\n",
    "## deconvolution function while running only one optimization\n",
    "def deconvolute_opt(solution, dim, num_restart, num_selection):\n",
    "    xy = np.zeros((num_restart, dim + 1))\n",
    "    for count in range(len(solution)):\n",
    "        xy[count, :-1] = solution[count].x\n",
    "        xy[count, -1] = solution[count].fun\n",
    "    select_xy = xy[:num_selection, :]\n",
    "    return select_xy\n",
    "## function for experiment selection while running only one optimization\n",
    "def exp_sel(y, FR, y_target, policy, num_selection, num_models = 20, num_restart = 1):\n",
    "    ## dataset and preprocessing needed to be able to train the model\n",
    "    y_norm = non_dim_y(y)\n",
    "    combination_true = rel_PLQY(y[:, [0]], y[:, 1:]) / 2.0\n",
    "    obj_fun_best = best_target_data(combination_true, y_target)\n",
    "    # a = y.shape[0]\n",
    "    # x_norm = non_dim_x(FR[:a, :])\n",
    "    x_norm = non_dim_x(FR)\n",
    "    x_train, y_train, x_valid, y_valid = split_set(x_norm, y_norm)\n",
    "    ## train the ENN\n",
    "    start_ML = time()\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    arthitecture = structure_info(num_models)\n",
    "    result_aggregate_ML = Parallel(n_jobs = -2)(delayed(parallel_train_ML)(x_train, y_train, x_valid, y_valid, arthitecture[i]) for i in range(num_models))\n",
    "    ensemble, history = deconvolute_ML(result_aggregate_ML)\n",
    "    end_ML = time()\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"The execution time to learn the ensemble NN is {str(end_ML - start_ML)} seconds.\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    ## optimization and achive the best input variables needed for the future experiments\n",
    "    start_opt = time()\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "    result_aggregate_opt = Parallel(n_jobs = -2)(delayed(parallel_opt)(ensemble, y_target, obj_fun_best, policy, x_norm.shape[1]) for i in range(num_restart))\n",
    "    xy_best = deconvolute_opt(result_aggregate_opt, x_norm.shape[1], num_restart, num_selection)\n",
    "    # sort based on the temperature\n",
    "    x_best = xy_best[:, :-1]\n",
    "    y_best = xy_best[:, [-1]]\n",
    "    end_opt = time()\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    print(f\"The execution time for optimization is {str(end_opt - start_opt)} seconds.\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    return x_best, y_best, result_aggregate_opt, ensemble, history, x_train, y_train, x_valid, y_valid, arthitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a52dbb",
   "metadata": {},
   "source": [
    "### Master Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(path):\n",
    "    files_all = os.listdir(path)\n",
    "    count_ABS = 0\n",
    "    count_PL = 0\n",
    "    for file in files_all:\n",
    "        if (file.startswith(\"Abs\")):\n",
    "            count_ABS = count_ABS + 1\n",
    "        elif (file.startswith(\"PL\")):\n",
    "            count_PL = count_PL + 1\n",
    "    return count_ABS, count_PL\n",
    "def x_dim(x_norm, FR, path):\n",
    "    x_final = np.zeros((x_norm.shape[0], x_norm.shape[1] + 3))\n",
    "    x_final[:, 0] = (x_norm[:, 0] * (255 - 160)) + 160\n",
    "    x_final[:, 1] = (x_norm[:, 1] * (50 - 45)) + 45\n",
    "    x_final[:, 2] = (x_norm[:, 2] * (30 - 10)) + 10\n",
    "    x_final[:, 3] = (x_norm[:, 3] * (30 - 10)) + 10\n",
    "    x_final[:, 4] = (x_norm[:, 4] * (70 - 20)) + 20\n",
    "    x_final[:, 5] = (x_norm[:, 5] * (80 - 25)) + 25\n",
    "    x_final[:, 6] = (x_norm[:, 6] * (40 - 10)) + 10\n",
    "    x_final[:, 7] = (x_norm[:, 7] * (50 - 15)) + 15\n",
    "    x_final[:, 8] = (x_norm[:, 8] * (50 - 15)) + 15\n",
    "    # third and second last columns\n",
    "    x_final[:, -3] = (1.2 * np.sum(x_final[:, 1:-3], axis = 1) * 7.46) / 1000 # x10 = MFC\n",
    "    x_final[:, -2] = 3 # x11 = 3 always\n",
    "    # last column is still 0 but after concatenating, we will apply the washing after every 6 rows\n",
    "    updated_FR = np.concatenate((FR, x_final), axis = 0) ## concat previous and new input parameters\n",
    "    # last column: 1 in the rows divisible by 7 for washing otherwise 0\n",
    "    for i in range(updated_FR.shape[0]):\n",
    "        if ((i + 1) % 7 == 0):\n",
    "            updated_FR[i, -1] = 1\n",
    "    np.savetxt(path + \"//\" + \"FR.csv\", updated_FR, delimiter = \",\", fmt=\"%1.3f\") # overwrite it into the existing FR.csv file\n",
    "    return x_final, updated_FR\n",
    "## master function\n",
    "## policy can be [\"MV\", 0], [\"EPLT\", 0], [\"EI\", 0], or [\"UCB\", 1. / math.sqrt(2)]\n",
    "# def master_code(y_target = 1., policy = [\"MV\", 0], num_selection = 1, exp_budget = 160): # if MV is used\n",
    "# def master_code(y_target = 1., policy = [\"EPLT\", 0], num_selection = 1, exp_budget = 160): # if EPLT is used\n",
    "# def master_code(y_target = 1., policy = [\"EI\", 0], num_selection = 1, exp_budget = 160): # if EI is used\n",
    "# def master_code(y_target = 1., policy = [\"EI\", 0], num_selection = 1, exp_budget = 13): # if EI without prior knowledge is used; initiated with 3 random experimental condition\n",
    "def master_code(y_target = 1., policy = [\"UCB\", 1. / math.sqrt(2)], num_selection = 1, exp_budget = 160): # if UCB is used\n",
    "    loc = input(\"Please enter the path for the directory that includes files: \")\n",
    "    print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "    num_files_old = 0\n",
    "    while True:\n",
    "        num_ABS, num_PL = count_files(loc)\n",
    "        if (num_ABS == num_PL):\n",
    "            num_files = num_PL\n",
    "        if (num_files > num_files_old + num_selection - 1) and (num_files < exp_budget):\n",
    "            print(\"New run starts now:\")\n",
    "            num_files_old = num_files\n",
    "            WL_ABS, WL_PL, DR_ABS, DR_PL, LR, ABS_all, PL_all, ABS_names, PL_names, FR = read_files(loc)\n",
    "            y_abs = spectra_extract_ABS(WL_ABS, DR_ABS, LR, ABS_all, ABS_names)\n",
    "            y_areas = spectra_extract_PL(WL_PL, DR_PL, PL_all, PL_names)\n",
    "            y = np.concatenate((y_abs, y_areas[:, 1:]), axis = 1)\n",
    "            # y1, y2, y3, y = rel_PLQY(y_abs, y_areas)\n",
    "            # y_final = y2 + y3\n",
    "            new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure = exp_sel(y, FR, y_target, policy, num_selection)\n",
    "            new_FR, updated_FR = x_dim(new_x, FR, loc)\n",
    "        elif (num_files >= exp_budget):\n",
    "            print(\"The experimental budget is reached!\")\n",
    "            break       \n",
    "#     return new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure, new_FR, updated_FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85383c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_x, new_y, solution_all, ensemble, history, x_train, y_train, x_valid, y_valid, structure, new_FR, updated_FR = master_code()\n",
    "master_code()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
